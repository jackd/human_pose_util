import os
import tensorflow as tf
import numpy as np

_int_dtypes = set([np.uint8, np.int8, np.int16, np.int32, np.int64])
_float_dtypes = set([np.float16, np.float32, np.float64])
_dtypes = _int_dtypes.union(_float_dtypes)


class ProxyBar(object):
    """
    Proxy class for progress bars.

    Does nothing. Useful for situations like:
    ```
    bar = ProxyBar() if n is None else IncrementalBar(max=n)
    bar.next()
    bar.next()
    bar.finish()
    ```
    """
    def next(self):
        pass

    def finish(self):
        pass


class FeatureSpec(object):
    """Specification for a tfrecords feature."""
    def __init__(self, key, shape, dtype):
        self.key = key
        self.shape = shape
        if hasattr(dtype, 'as_numpy_dtype'):
            dtype = dtype.as_numpy_dtype
        if dtype not in _dtypes:
            raise ValueError('dtype %s not in allowable dtypes' % str(dtype))
        self.dtype = dtype


def _feature_specs(feature_specs):
    if isinstance(feature_specs, FeatureSpec):
        feature_specs = [feature_specs]
    elif not (hasattr(feature_specs, '__iter__') and
              all([isinstance(spec, FeatureSpec) for spec in feature_specs])):
        raise ValueError(
            'feature_specs must be a `FeatureSpec` or an iterable of '
            '`FeatureSpec`s')
    return feature_specs


def write_records(path, feature_specs, examples_fn):
    """
    Write data generated by examples_fn to a tfrecords file at path.

    Args:
        path: path to save file to
        feature_specs: a `FeatureSpec` or iterable of `FeatureSpec`s
        examples_fn: function producing an iterable of examples.
    """
    from progress.bar import IncrementalBar
    feature_specs = _feature_specs(feature_specs)
    examples = examples_fn()
    if hasattr(examples, '__len__'):
        bar = IncrementalBar(max=len(examples))
    else:
        bar = ProxyBar()
    try:
        feature_fns = {}
        for spec in feature_specs:
            k = spec.key
            if spec.dtype in _float_dtypes:
                feature_fns[k] = lambda feature, value: \
                    feature.float_list.value.extend(value)
            elif spec.dtype in _int_dtypes:
                feature_fns[k] = lambda feature, value: \
                    feature.int64_list.value.extend(value)
            else:
                raise RuntimeError('Invalid dtype: %s' % str(spec.dtype))
        with tf.python_io.TFRecordWriter(path) as writer:
            print('Creating tf_records: %s' % path)
            for example in examples:
                ex = tf.train.Example()
                features = ex.features
                for spec, val in zip(feature_specs, example):
                    k = spec.key
                    assert(val.shape == spec.shape)
                    assert(val.dtype == spec.dtype)
                    if len(val.shape) > 1:
                        val = np.reshape(val, (-1,))
                    feature_fns[k](features.feature[k], val)
                writer.write(ex.SerializeToString())
                bar.next()
            bar.finish()
    except Exception:
        print('Writing dataset failed. Deleting...')
        os.remove(path)
        raise


def load_dataset(paths, feature_specs):
    """
    Create a dataset from tfrecord file(s) and parse the result.

    Args:
        paths: string or iterable of strings of addresses of tfrecords files
        feature_specs: `FeatureSpec` of iterable of `FeatureSpec`s used to
            parse the dataset. Should be the same to those used in
            `write_records` (order may vary).
    Returns:
        dataset with elements corresponding to feature_specs
    """
    if isinstance(paths, (str, unicode)):
        paths = [paths]
    for path in paths:
        if not os.path.isfile(path):
            raise ValueError('No tfrecords file at path: %s' % path)
    feature_specs = _feature_specs(feature_specs)

    def parse_example(example_proto):
        features = {
            spec.key: tf.FixedLenFeature(
                np.prod(spec.shape, dtype=int), spec.dtype)
            for spec in feature_specs
        }

        parsed_features = tf.parse_single_example(example_proto, features)
        features = []
        for spec in feature_specs:
            feature = parsed_features[spec.key]
            if len(spec.shape) > 1:
                feature = tf.reshape(feature, spec.shape)
            if feature.dtype.as_numpy_dtype != spec.dtype:
                feature = tf.cast(feature, spec.dtype)
            features.append(feature)
        return features
    return tf.data.TFRecordDataset(paths).map(parse_example)


def get_dataset(path, feature_specs, examples_fn):
    """
    Get a dataset from file, writing it first if it does not exist.

    See `write_records` and `load_dataset`.
    """
    if not os.path.isfile(path):
        write_records(path, feature_specs, examples_fn)
    return load_dataset(path, feature_specs)


def set_batch_size(tensor, batch_size):
    """Se the batch size of the given tensor."""
    shape = tensor.shape.as_list()
    assert(shape[0] is None or shape[0] == batch_size)
    shape[0] = batch_size
    tensor.set_shape(shape)

# def examples_to_tf(examples, features):
#     """Convert `human_pose_util.dataset.Example`s to `tf.train.Example`s."""
#     feature_adder = {}
#     for k, v in features.items():
#         # FixedLenFeature
#         # FixedLenSequenceFeature
#         # VarLenFeature
#         # SparseFeature
#         if not isinstance(v, (tf.FixedLenFeature,
#                               tf.FixedLenSequenceFeature,
#                               tf.VarLenFeature)):
#             raise NotImplementedError()
#         is_list = len(v.shape) > 0 or isinstance(
#                 v, tf.FixedLenSequenceFeature)
#         if v.dtype == tf.float32:
#             if is_list:
#                 feature_adder[k] = lambda f, data: \
#                     f.float32_list.extend(data)
#             else:
#                 feature_adder[k] = lambda f, data: \
#                     f.float32_list.append(data)
#         elif v.dtype == tf.int64:
#             if is_list:
#                 feature_adder[k] = lambda f, data: \
#                     f.int64_list.extend(data)
#             else:
#                 feature_adder[k] = lambda f, data: \
#                     f.int64_list.append(data)
#         elif v.dtype == tf.string:
#             assert(is_list)
#             feature_adder[k] = lambda f, data: \
#                 f.bytes_list.extend(data.tostring())
#         else:
#             raise NotImplementedError(
#                 'No FixedLenFeature for dtype %s' % str(v.dtype))
#     for example in examples:
#         tf_example = tf.train.Example()
#         for k in features:
#             feature_adder[k](tf_example.features[k], example[k])
#             yield example
#
#
# def dataset_to_tf_records(examples, features, filename):
#     with tf.python_io.TFRecordWriter(filename) as writer:
#         for example in examples_to_tf(examples, features):
#             writer.write(example.SerializeToString())
